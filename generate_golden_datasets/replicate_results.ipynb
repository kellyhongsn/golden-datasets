{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Golden Datasets] Results Replication\n",
    "\n",
    "This notebook demonstrates the simplified version of how to replicate our results.\n",
    "\n",
    "Using the English subset of the [multilingual Wikipedia dataset](https://huggingface.co/datasets/ellamind/wikipedia-2023-11-retrieval-multilingual-queries), Anthropic's `claude-3-5-sonnet`, and OpenAI's `text-embedding-3-small`, we will demonstrate the following:\n",
    "- Models have memorized public benchmarks\n",
    "- We are able to generate unseen queries that are representative of the ground truth dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install & Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from voyageai import Client as VoyageClient\n",
    "from openai import OpenAI as OpenAIClient\n",
    "from anthropic import Anthropic as AnthropicClient\n",
    "from llm_calls import *\n",
    "from utils import *\n",
    "from embedding_funcs import *\n",
    "from chroma_funcs import *\n",
    "from evaluation_funcs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load API Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Chroma Cloud, you can sign up for a Chroma Cloud account [here](https://www.trychroma.com/) and create a new database. If you want to use local Chroma, skip this step and simply input `OPENAI_API_KEY` and `CLAUDE_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma Cloud\n",
    "CHROMA_TENANT = \"YOUR CHROMA TENANT ID\"\n",
    "X_CHROMA_TOKEN = \"YOUR CHROMA API KEY\"\n",
    "DATABASE_NAME = \"YOUR CHROMA DATABASE NAME\"\n",
    "\n",
    "# Embedding Model\n",
    "OPENAI_API_KEY = \"YOUR OPENAI API KEY\"\n",
    "\n",
    "# LLM\n",
    "CLAUDE_API_KEY = \"YOUR CLAUDE API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Set Clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use our API keys to initialize the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.HttpClient(\n",
    "  ssl=True,\n",
    "  host='api.trychroma.com',\n",
    "  tenant=CHROMA_TENANT,\n",
    "  database=DATABASE_NAME,\n",
    "  headers={\n",
    "    'x-chroma-token': X_CHROMA_TOKEN\n",
    "  }\n",
    ")\n",
    "\n",
    "# If you want to use the local Chroma instead, uncomment the following line:\n",
    "# chroma_client = chromadb.Client()\n",
    "\n",
    "openai_client = OpenAIClient(api_key=OPENAI_API_KEY)\n",
    "claude_client = AnthropicClient(api_key=CLAUDE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this simplified version, we'll use the English subset of the [multilingual Wikipedia dataset](https://huggingface.co/datasets/ellamind/wikipedia-2023-11-retrieval-multilingual-queries).\n",
    "\n",
    "We'll use the `test` split for this demonstration, which contains:\n",
    "- 1500 queries\n",
    "- 1500 query-corpus relevance judgments\n",
    "- 13500 corpus documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll load the queries, corpus, and query-corpus relevance judgments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_queries = datasets.load_dataset(\"ellamind/wikipedia-2023-11-retrieval-multilingual-queries\", \"en\")[\"test\"].to_pandas()\n",
    "wiki_corpus = datasets.load_dataset(\"ellamind/wikipedia-2023-11-retrieval-multilingual-corpus\", \"en\")[\"test\"].to_pandas()\n",
    "wiki_qrels = datasets.load_dataset(\"ellamind/wikipedia-2023-11-retrieval-multilingual-qrels\", \"en\")[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this specific dataset, the query-corpus relevance judgements include distractors as indicated by `score`s of `0.5` and target matches as indicated by `score`s of `1.0`.\n",
    "\n",
    "We'll filter the query-corpus relevance judgments to only include target matches. Then, we'll combine the queries, corpus, and query-corpus relevance judgments into a single dataframe for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_qrels = wiki_qrels[wiki_qrels[\"score\"] == 1.0]\n",
    "\n",
    "wiki_qrels = combined_datasets_dataframes(wiki_queries, wiki_corpus, wiki_qrels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embed Corpus & Store in Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a Chroma collection to store our corpus embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_collection = chroma_client.get_or_create_collection(\n",
    "    name=\"wiki-text-embedding-3-small\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed the corpus using `text-embedding-3-small` and add to `wiki_collection` (we use batching and threading to speed up the process).\n",
    "\n",
    "We'll also create a lookup dictionary to store the corpus embeddings for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_corpus_ids = wiki_corpus[\"_id\"].tolist()\n",
    "wiki_corpus_texts = wiki_corpus[\"text\"].tolist()\n",
    "\n",
    "wiki_corpus_embeddings = openai_embed_in_batches(openai_client, wiki_corpus_texts, \"text-embedding-3-small\")\n",
    "\n",
    "collection_add_in_batches(wiki_collection, wiki_corpus_ids, wiki_corpus_texts, wiki_corpus_embeddings)\n",
    "\n",
    "wiki_corpus_lookup = {\n",
    "    id: {\n",
    "        \"text\": text,\n",
    "        \"embedding\": embedding\n",
    "    } for id, text, embedding in zip(wiki_corpus_ids, wiki_corpus_texts, wiki_corpus_embeddings)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Query Generation\n",
    "\n",
    "We will demonstrate that models have memorized public benchmarks with a naive query generation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 1500 queries, only including the corpus as context. The prompt can be found in `llm_calls.py` under `generate_query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_simple_generated_queries = []\n",
    "\n",
    "for _, row in tqdm(wiki_qrels.iterrows(), total=len(wiki_qrels), desc=\"Generating simple queries...\"):\n",
    "    corpus = row['corpus-text']\n",
    "    generated_query = generate_query(claude_client, corpus)\n",
    "    wiki_simple_generated_queries.append(generated_query)\n",
    "\n",
    "wiki_qrels[\"simple-generated-query-text\"] = wiki_simple_generated_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we embed the original queries and generated queries. We store the embeddings in a lookup dictionary as well for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_original_queries = wiki_qrels[\"query-text\"].tolist()\n",
    "wiki_query_ids = wiki_qrels[\"query-id\"].tolist()\n",
    "\n",
    "wiki_original_query_embeddings = openai_embed_in_batches(openai_client, wiki_original_queries, \"text-embedding-3-small\")\n",
    "wiki_simple_generated_query_embeddings = openai_embed_in_batches(openai_client, wiki_simple_generated_queries, \"text-embedding-3-small\")\n",
    "\n",
    "wiki_original_query_lookup = {\n",
    "    id: {\n",
    "        \"text\": text,\n",
    "        \"embedding\": embedding\n",
    "    } for id, text, embedding in zip(wiki_query_ids, wiki_original_queries, wiki_original_query_embeddings)\n",
    "}\n",
    "\n",
    "wiki_simple_generated_query_lookup = {\n",
    "    id: {\n",
    "        \"text\": text,\n",
    "        \"embedding\": embedding\n",
    "    } for id, text, embedding in zip(wiki_query_ids, wiki_simple_generated_queries, wiki_simple_generated_query_embeddings)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our embeddings are computed, we can compare the cosine similarity between the original queries and generated queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_query_query_scores = score_query_query(wiki_qrels, wiki_original_query_lookup, wiki_simple_generated_query_lookup)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.hist(wiki_query_query_scores[\"query-query-score\"], bins=30, alpha=0.5,  edgecolor='black', label=\"Score\", range=(0, 1), density=True)\n",
    "\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Normalized Frequency\")\n",
    "plt.title(\"text-embedding-3-small\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our generated queries are very similar to the original queries, and we can investigate this further here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_query_query_scores.sort_values(by=\"query-query-score\", ascending=False, inplace=True)\n",
    "\n",
    "for i, row in wiki_query_query_scores.head(10).iterrows():\n",
    "    print(f\"Score: {row['query-query-score']:.4f}\")\n",
    "    print(f\"Original Query: {row['query-text']}\")\n",
    "    print(f\"Generated Query: {row['simple-generated-query-text']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distinct Query Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since models have memorized these public benchmarks, we will generate unseen queries by explicitely prompting the model to generate a distinct query. \n",
    "\n",
    "Then, we will demonstrate that these newly generated distinct queries are also representative of the ground truth dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate 1500 queries, now including both the original query and the corpus as context. The prompt can be found in `llm_calls.py` under `generate_query_with_example`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_distinct_generated_queries = []\n",
    "\n",
    "for _, row in tqdm(wiki_qrels.iterrows(), total=len(wiki_qrels), desc=\"Generating distinct queries...\"):\n",
    "    query = row['query-text']\n",
    "    corpus = row['corpus-text']\n",
    "    generated_query = generate_query_with_example(claude_client, query, corpus)\n",
    "    wiki_distinct_generated_queries.append(generated_query)\n",
    "\n",
    "wiki_qrels[\"distinct-generated-query-text\"] = wiki_distinct_generated_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We embed the newly generated queries and store them in a lookup dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_distinct_generated_query_embeddings = openai_embed_in_batches(openai_client, wiki_distinct_generated_queries, \"text-embedding-3-small\")\n",
    "\n",
    "wiki_distinct_generated_query_lookup = {\n",
    "    id: {\n",
    "        \"text\": text,\n",
    "        \"embedding\": embedding\n",
    "    } for id, text, embedding in zip(wiki_query_ids, wiki_distinct_generated_queries, wiki_distinct_generated_query_embeddings)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the retrieval task across both the generated and original queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 3, 5, 10]\n",
    "\n",
    "wiki_distinct_gen_results = get_results(wiki_collection, wiki_distinct_generated_queries, wiki_corpus_ids, wiki_distinct_generated_query_embeddings)\n",
    "wiki_distinct_gen_metrics = evaluate(k_values, wiki_qrels, wiki_distinct_gen_results)\n",
    "\n",
    "wiki_original_results = get_results(wiki_collection, wiki_original_queries, wiki_corpus_ids, wiki_original_query_embeddings)\n",
    "wiki_original_metrics = evaluate(k_values, wiki_qrels, wiki_original_results)\n",
    "\n",
    "wiki_metrics = [wiki_distinct_gen_metrics, wiki_original_metrics]\n",
    "labels = [\"Generated\", \"Original\"]\n",
    "\n",
    "comparison_df = create_comparison_dataframe(wiki_metrics, labels)\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the cosine similarity distributions here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_distinct_gen_scores = score_query_corpus(wiki_qrels, wiki_distinct_generated_query_lookup, wiki_corpus_lookup)\n",
    "wiki_original_scores = score_query_corpus(wiki_qrels, wiki_original_query_lookup, wiki_corpus_lookup)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.hist(wiki_distinct_gen_scores[\"query-corpus-score\"], bins=30, alpha=0.5,  edgecolor='black', label=\"Original\", range=(0, 1), density=True)\n",
    "plt.hist(wiki_original_scores[\"query-corpus-score\"], bins=30, alpha=0.5, edgecolor='black', label=\"Generated\", range=(0, 1), density=True)\n",
    "\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Normalized Frequency\")\n",
    "\n",
    "plt.title(\"text-embedding-3-small (1500 Queries, 13500 Documents)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
